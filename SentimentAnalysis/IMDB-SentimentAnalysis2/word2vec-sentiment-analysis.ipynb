{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def review_to_wordlist(raw_reviews, remove_stopwords=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML Tags\n",
    "    review_text = BeautifulSoup(raw_reviews, \"lxml\").get_text()\n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    # 6. Lemmatize the words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    \n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer, True)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker']\n",
      "['maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[0])\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shreyansh singh\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-08-07 14:48:53,200 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-08-07 14:48:53,223 : INFO : collecting all words and their counts\n",
      "2017-08-07 14:48:53,225 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-08-07 14:48:53,278 : INFO : PROGRESS: at sentence #10000, processed 114931 words, keeping 17627 word types\n",
      "2017-08-07 14:48:53,323 : INFO : PROGRESS: at sentence #20000, processed 228988 words, keeping 24797 word types\n",
      "2017-08-07 14:48:53,352 : INFO : PROGRESS: at sentence #30000, processed 339533 words, keeping 29883 word types\n",
      "2017-08-07 14:48:53,393 : INFO : PROGRESS: at sentence #40000, processed 453983 words, keeping 34196 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-07 14:48:53,431 : INFO : PROGRESS: at sentence #50000, processed 565006 words, keeping 37609 word types\n",
      "2017-08-07 14:48:53,476 : INFO : PROGRESS: at sentence #60000, processed 676637 words, keeping 40571 word types\n",
      "2017-08-07 14:48:53,512 : INFO : PROGRESS: at sentence #70000, processed 789005 words, keeping 43180 word types\n",
      "2017-08-07 14:48:53,540 : INFO : PROGRESS: at sentence #80000, processed 899771 words, keeping 45561 word types\n",
      "2017-08-07 14:48:53,587 : INFO : PROGRESS: at sentence #90000, processed 1013453 words, keeping 47982 word types\n",
      "2017-08-07 14:48:53,625 : INFO : PROGRESS: at sentence #100000, processed 1125135 words, keeping 50054 word types\n",
      "2017-08-07 14:48:53,663 : INFO : PROGRESS: at sentence #110000, processed 1236261 words, keeping 51928 word types\n",
      "2017-08-07 14:48:53,700 : INFO : PROGRESS: at sentence #120000, processed 1348541 words, keeping 53966 word types\n",
      "2017-08-07 14:48:53,737 : INFO : PROGRESS: at sentence #130000, processed 1461911 words, keeping 55694 word types\n",
      "2017-08-07 14:48:53,776 : INFO : PROGRESS: at sentence #140000, processed 1568503 words, keeping 57193 word types\n",
      "2017-08-07 14:48:53,815 : INFO : PROGRESS: at sentence #150000, processed 1682622 words, keeping 58902 word types\n",
      "2017-08-07 14:48:53,853 : INFO : PROGRESS: at sentence #160000, processed 1794988 words, keeping 60464 word types\n",
      "2017-08-07 14:48:53,893 : INFO : PROGRESS: at sentence #170000, processed 1907744 words, keeping 61924 word types\n",
      "2017-08-07 14:48:53,929 : INFO : PROGRESS: at sentence #180000, processed 2018412 words, keeping 63343 word types\n",
      "2017-08-07 14:48:53,971 : INFO : PROGRESS: at sentence #190000, processed 2131820 words, keeping 64641 word types\n",
      "2017-08-07 14:48:53,987 : INFO : PROGRESS: at sentence #200000, processed 2245187 words, keeping 65934 word types\n",
      "2017-08-07 14:48:54,043 : INFO : PROGRESS: at sentence #210000, processed 2357380 words, keeping 67237 word types\n",
      "2017-08-07 14:48:54,083 : INFO : PROGRESS: at sentence #220000, processed 2470883 words, keeping 68544 word types\n",
      "2017-08-07 14:48:54,120 : INFO : PROGRESS: at sentence #230000, processed 2582936 words, keeping 69805 word types\n",
      "2017-08-07 14:48:54,161 : INFO : PROGRESS: at sentence #240000, processed 2697681 words, keeping 71014 word types\n",
      "2017-08-07 14:48:54,198 : INFO : PROGRESS: at sentence #250000, processed 2805851 words, keeping 72198 word types\n",
      "2017-08-07 14:48:54,235 : INFO : PROGRESS: at sentence #260000, processed 2916657 words, keeping 73325 word types\n",
      "2017-08-07 14:48:54,274 : INFO : PROGRESS: at sentence #270000, processed 3028489 words, keeping 74614 word types\n",
      "2017-08-07 14:48:54,302 : INFO : PROGRESS: at sentence #280000, processed 3142417 words, keeping 76216 word types\n",
      "2017-08-07 14:48:54,348 : INFO : PROGRESS: at sentence #290000, processed 3255495 words, keeping 77686 word types\n",
      "2017-08-07 14:48:54,371 : INFO : PROGRESS: at sentence #300000, processed 3368941 words, keeping 79018 word types\n",
      "2017-08-07 14:48:54,423 : INFO : PROGRESS: at sentence #310000, processed 3483356 words, keeping 80327 word types\n",
      "2017-08-07 14:48:54,461 : INFO : PROGRESS: at sentence #320000, processed 3597218 words, keeping 81655 word types\n",
      "2017-08-07 14:48:54,500 : INFO : PROGRESS: at sentence #330000, processed 3709098 words, keeping 82877 word types\n",
      "2017-08-07 14:48:54,541 : INFO : PROGRESS: at sentence #340000, processed 3825874 words, keeping 84127 word types\n",
      "2017-08-07 14:48:54,581 : INFO : PROGRESS: at sentence #350000, processed 3938196 words, keeping 85272 word types\n",
      "2017-08-07 14:48:54,619 : INFO : PROGRESS: at sentence #360000, processed 4049743 words, keeping 86443 word types\n",
      "2017-08-07 14:48:54,666 : INFO : PROGRESS: at sentence #370000, processed 4164653 words, keeping 87555 word types\n",
      "2017-08-07 14:48:54,708 : INFO : PROGRESS: at sentence #380000, processed 4278559 words, keeping 88725 word types\n",
      "2017-08-07 14:48:54,756 : INFO : PROGRESS: at sentence #390000, processed 4394875 words, keeping 89754 word types\n",
      "2017-08-07 14:48:54,798 : INFO : PROGRESS: at sentence #400000, processed 4507279 words, keeping 90763 word types\n",
      "2017-08-07 14:48:54,839 : INFO : PROGRESS: at sentence #410000, processed 4619254 words, keeping 91727 word types\n",
      "2017-08-07 14:48:54,873 : INFO : PROGRESS: at sentence #420000, processed 4731336 words, keeping 92759 word types\n",
      "2017-08-07 14:48:54,917 : INFO : PROGRESS: at sentence #430000, processed 4846238 words, keeping 93779 word types\n",
      "2017-08-07 14:48:54,937 : INFO : PROGRESS: at sentence #440000, processed 4960659 words, keeping 94753 word types\n",
      "2017-08-07 14:48:54,997 : INFO : PROGRESS: at sentence #450000, processed 5073240 words, keeping 95883 word types\n",
      "2017-08-07 14:48:55,039 : INFO : PROGRESS: at sentence #460000, processed 5191383 words, keeping 96935 word types\n",
      "2017-08-07 14:48:55,069 : INFO : PROGRESS: at sentence #470000, processed 5306594 words, keeping 97780 word types\n",
      "2017-08-07 14:48:55,115 : INFO : PROGRESS: at sentence #480000, processed 5418049 words, keeping 98709 word types\n",
      "2017-08-07 14:48:55,160 : INFO : PROGRESS: at sentence #490000, processed 5532990 words, keeping 99718 word types\n",
      "2017-08-07 14:48:55,200 : INFO : PROGRESS: at sentence #500000, processed 5644988 words, keeping 100612 word types\n",
      "2017-08-07 14:48:55,224 : INFO : PROGRESS: at sentence #510000, processed 5758934 words, keeping 101546 word types\n",
      "2017-08-07 14:48:55,281 : INFO : PROGRESS: at sentence #520000, processed 5871985 words, keeping 102445 word types\n",
      "2017-08-07 14:48:55,324 : INFO : PROGRESS: at sentence #530000, processed 5985009 words, keeping 103247 word types\n",
      "2017-08-07 14:48:55,366 : INFO : PROGRESS: at sentence #540000, processed 6098542 words, keeping 104112 word types\n",
      "2017-08-07 14:48:55,417 : INFO : PROGRESS: at sentence #550000, processed 6212802 words, keeping 104980 word types\n",
      "2017-08-07 14:48:55,462 : INFO : PROGRESS: at sentence #560000, processed 6324873 words, keeping 105844 word types\n",
      "2017-08-07 14:48:55,488 : INFO : PROGRESS: at sentence #570000, processed 6440294 words, keeping 106634 word types\n",
      "2017-08-07 14:48:55,539 : INFO : PROGRESS: at sentence #580000, processed 6552396 words, keeping 107512 word types\n",
      "2017-08-07 14:48:55,584 : INFO : PROGRESS: at sentence #590000, processed 6666834 words, keeping 108348 word types\n",
      "2017-08-07 14:48:55,604 : INFO : PROGRESS: at sentence #600000, processed 6779294 words, keeping 109065 word types\n",
      "2017-08-07 14:48:55,663 : INFO : PROGRESS: at sentence #610000, processed 6891185 words, keeping 109939 word types\n",
      "2017-08-07 14:48:55,711 : INFO : PROGRESS: at sentence #620000, processed 7005528 words, keeping 110684 word types\n",
      "2017-08-07 14:48:55,750 : INFO : PROGRESS: at sentence #630000, processed 7118821 words, keeping 111457 word types\n",
      "2017-08-07 14:48:55,792 : INFO : PROGRESS: at sentence #640000, processed 7230329 words, keeping 112263 word types\n",
      "2017-08-07 14:48:55,833 : INFO : PROGRESS: at sentence #650000, processed 7345230 words, keeping 113043 word types\n",
      "2017-08-07 14:48:55,853 : INFO : PROGRESS: at sentence #660000, processed 7457609 words, keeping 113792 word types\n",
      "2017-08-07 14:48:55,911 : INFO : PROGRESS: at sentence #670000, processed 7569373 words, keeping 114490 word types\n",
      "2017-08-07 14:48:55,957 : INFO : PROGRESS: at sentence #680000, processed 7683701 words, keeping 115201 word types\n",
      "2017-08-07 14:48:55,999 : INFO : PROGRESS: at sentence #690000, processed 7796432 words, keeping 115978 word types\n",
      "2017-08-07 14:48:56,020 : INFO : PROGRESS: at sentence #700000, processed 7911989 words, keeping 116790 word types\n",
      "2017-08-07 14:48:56,076 : INFO : PROGRESS: at sentence #710000, processed 8024618 words, keeping 117443 word types\n",
      "2017-08-07 14:48:56,116 : INFO : PROGRESS: at sentence #720000, processed 8138460 words, keeping 118068 word types\n",
      "2017-08-07 14:48:56,162 : INFO : PROGRESS: at sentence #730000, processed 8253065 words, keeping 118801 word types\n",
      "2017-08-07 14:48:56,208 : INFO : PROGRESS: at sentence #740000, processed 8364416 words, keeping 119515 word types\n",
      "2017-08-07 14:48:56,237 : INFO : PROGRESS: at sentence #750000, processed 8475132 words, keeping 120142 word types\n",
      "2017-08-07 14:48:56,294 : INFO : PROGRESS: at sentence #760000, processed 8585172 words, keeping 120777 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-07 14:48:56,335 : INFO : PROGRESS: at sentence #770000, processed 8700018 words, keeping 121550 word types\n",
      "2017-08-07 14:48:56,369 : INFO : PROGRESS: at sentence #780000, processed 8816138 words, keeping 122249 word types\n",
      "2017-08-07 14:48:56,414 : INFO : PROGRESS: at sentence #790000, processed 8930668 words, keeping 122913 word types\n",
      "2017-08-07 14:48:56,446 : INFO : collected 123351 word types from a corpus of 8993057 raw words and 795538 sentences\n",
      "2017-08-07 14:48:56,448 : INFO : Loading a fresh vocabulary\n",
      "2017-08-07 14:48:56,639 : INFO : min_count=40 retains 16340 unique words (13% of original 123351, drops 107011)\n",
      "2017-08-07 14:48:56,640 : INFO : min_count=40 leaves 8433955 word corpus (93% of original 8993057, drops 559102)\n",
      "2017-08-07 14:48:56,733 : INFO : deleting the raw counts dictionary of 123351 items\n",
      "2017-08-07 14:48:56,744 : INFO : sample=0.001 downsamples 24 most-common words\n",
      "2017-08-07 14:48:56,745 : INFO : downsampling leaves estimated 8064172 word corpus (95.6% of prior 8433955)\n",
      "2017-08-07 14:48:56,747 : INFO : estimated required memory for 16340 words and 300 dimensions: 47386000 bytes\n",
      "2017-08-07 14:48:56,829 : INFO : resetting layer weights\n",
      "2017-08-07 14:48:57,120 : INFO : training model with 4 workers on 16340 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-08-07 14:48:58,129 : INFO : PROGRESS: at 1.38% examples, 549720 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:48:59,136 : INFO : PROGRESS: at 2.84% examples, 564615 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:00,137 : INFO : PROGRESS: at 4.30% examples, 570470 words/s, in_qsize 6, out_qsize 0\n",
      "2017-08-07 14:49:01,143 : INFO : PROGRESS: at 5.72% examples, 571137 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:02,140 : INFO : PROGRESS: at 7.07% examples, 563250 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:03,158 : INFO : PROGRESS: at 8.40% examples, 558735 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:04,160 : INFO : PROGRESS: at 9.79% examples, 559022 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:05,184 : INFO : PROGRESS: at 11.25% examples, 560883 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:06,186 : INFO : PROGRESS: at 12.70% examples, 564149 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:07,187 : INFO : PROGRESS: at 14.18% examples, 567546 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:08,201 : INFO : PROGRESS: at 15.65% examples, 569558 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:09,195 : INFO : PROGRESS: at 17.07% examples, 569838 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:10,225 : INFO : PROGRESS: at 18.53% examples, 570206 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:11,250 : INFO : PROGRESS: at 20.04% examples, 571870 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:12,270 : INFO : PROGRESS: at 21.51% examples, 572594 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:13,280 : INFO : PROGRESS: at 22.99% examples, 573100 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:14,282 : INFO : PROGRESS: at 24.45% examples, 573547 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:15,289 : INFO : PROGRESS: at 25.83% examples, 572445 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:16,305 : INFO : PROGRESS: at 27.29% examples, 572529 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:17,321 : INFO : PROGRESS: at 28.77% examples, 573320 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:18,340 : INFO : PROGRESS: at 30.25% examples, 574250 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:19,354 : INFO : PROGRESS: at 31.66% examples, 573773 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:20,367 : INFO : PROGRESS: at 32.99% examples, 572041 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:21,366 : INFO : PROGRESS: at 34.44% examples, 572583 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:22,373 : INFO : PROGRESS: at 35.94% examples, 573565 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:23,398 : INFO : PROGRESS: at 37.36% examples, 573255 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:24,391 : INFO : PROGRESS: at 38.82% examples, 573848 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:25,419 : INFO : PROGRESS: at 40.30% examples, 574338 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:26,412 : INFO : PROGRESS: at 41.80% examples, 575187 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:27,430 : INFO : PROGRESS: at 43.25% examples, 575230 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:28,461 : INFO : PROGRESS: at 44.72% examples, 574778 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:29,475 : INFO : PROGRESS: at 46.19% examples, 575209 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:30,475 : INFO : PROGRESS: at 47.68% examples, 575924 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:31,482 : INFO : PROGRESS: at 49.12% examples, 575855 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:32,494 : INFO : PROGRESS: at 50.56% examples, 575958 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:33,504 : INFO : PROGRESS: at 51.94% examples, 575573 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:34,516 : INFO : PROGRESS: at 53.38% examples, 575526 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:35,533 : INFO : PROGRESS: at 54.60% examples, 573141 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:36,543 : INFO : PROGRESS: at 55.78% examples, 570496 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:37,543 : INFO : PROGRESS: at 57.11% examples, 569581 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:38,572 : INFO : PROGRESS: at 58.43% examples, 568529 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:39,632 : INFO : PROGRESS: at 59.55% examples, 564886 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:40,638 : INFO : PROGRESS: at 60.39% examples, 559647 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:41,643 : INFO : PROGRESS: at 61.53% examples, 557269 words/s, in_qsize 8, out_qsize 1\n",
      "2017-08-07 14:49:42,643 : INFO : PROGRESS: at 62.90% examples, 557034 words/s, in_qsize 7, out_qsize 1\n",
      "2017-08-07 14:49:43,669 : INFO : PROGRESS: at 64.22% examples, 556128 words/s, in_qsize 6, out_qsize 1\n",
      "2017-08-07 14:49:44,676 : INFO : PROGRESS: at 65.54% examples, 555462 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:45,689 : INFO : PROGRESS: at 66.78% examples, 554015 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:46,698 : INFO : PROGRESS: at 68.01% examples, 552877 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:47,750 : INFO : PROGRESS: at 68.92% examples, 548647 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:48,767 : INFO : PROGRESS: at 69.84% examples, 545116 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:49,775 : INFO : PROGRESS: at 70.80% examples, 542002 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:50,820 : INFO : PROGRESS: at 71.59% examples, 537460 words/s, in_qsize 8, out_qsize 1\n",
      "2017-08-07 14:49:51,821 : INFO : PROGRESS: at 72.49% examples, 534325 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:52,841 : INFO : PROGRESS: at 73.35% examples, 530819 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:53,849 : INFO : PROGRESS: at 74.18% examples, 527227 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:54,865 : INFO : PROGRESS: at 75.20% examples, 525077 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:55,869 : INFO : PROGRESS: at 76.39% examples, 524344 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:49:56,893 : INFO : PROGRESS: at 77.61% examples, 523613 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:49:57,901 : INFO : PROGRESS: at 78.90% examples, 523485 words/s, in_qsize 8, out_qsize 1\n",
      "2017-08-07 14:49:58,914 : INFO : PROGRESS: at 80.16% examples, 523164 words/s, in_qsize 8, out_qsize 1\n",
      "2017-08-07 14:49:59,922 : INFO : PROGRESS: at 81.43% examples, 522887 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:50:00,932 : INFO : PROGRESS: at 82.74% examples, 522756 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:50:01,941 : INFO : PROGRESS: at 84.06% examples, 522775 words/s, in_qsize 7, out_qsize 1\n",
      "2017-08-07 14:50:02,963 : INFO : PROGRESS: at 85.31% examples, 522286 words/s, in_qsize 6, out_qsize 1\n",
      "2017-08-07 14:50:03,987 : INFO : PROGRESS: at 86.53% examples, 521522 words/s, in_qsize 7, out_qsize 2\n",
      "2017-08-07 14:50:04,991 : INFO : PROGRESS: at 87.61% examples, 520269 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-07 14:50:05,999 : INFO : PROGRESS: at 88.92% examples, 520341 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:50:07,002 : INFO : PROGRESS: at 90.20% examples, 520310 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-07 14:50:08,010 : INFO : PROGRESS: at 91.46% examples, 520108 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:50:09,062 : INFO : PROGRESS: at 92.65% examples, 519216 words/s, in_qsize 7, out_qsize 1\n",
      "2017-08-07 14:50:10,070 : INFO : PROGRESS: at 93.69% examples, 517813 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:50:11,084 : INFO : PROGRESS: at 94.75% examples, 516532 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:50:12,094 : INFO : PROGRESS: at 95.99% examples, 516248 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:50:13,113 : INFO : PROGRESS: at 97.24% examples, 515935 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:50:14,129 : INFO : PROGRESS: at 98.50% examples, 515771 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:50:15,143 : INFO : PROGRESS: at 99.83% examples, 515946 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-07 14:50:15,239 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-08-07 14:50:15,268 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-08-07 14:50:15,279 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-08-07 14:50:15,285 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-08-07 14:50:15,287 : INFO : training on 44965285 raw words (40321011 effective words) took 78.2s, 515888 effective words/s\n",
      "2017-08-07 14:50:15,290 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-08-07 14:50:15,497 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-08-07 14:50:15,499 : INFO : not storing attribute syn0norm\n",
      "2017-08-07 14:50:15,500 : INFO : not storing attribute cum_table\n",
      "2017-08-07 14:50:16,537 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'london'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lady', 0.5552449226379395),\n",
       " ('men', 0.5232553482055664),\n",
       " ('woman', 0.5166194438934326),\n",
       " ('lad', 0.47608307003974915),\n",
       " ('mans', 0.46191689372062683),\n",
       " ('monk', 0.4449968934059143),\n",
       " ('guy', 0.42385101318359375),\n",
       " ('person', 0.4234864115715027),\n",
       " ('farmer', 0.41675615310668945),\n",
       " ('widow', 0.40869805216789246)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('latifah', 0.6391854286193848),\n",
       " ('princess', 0.6235027313232422),\n",
       " ('bride', 0.5827382802963257),\n",
       " ('prince', 0.5814833641052246),\n",
       " ('heiress', 0.5804899334907532),\n",
       " ('goddess', 0.5770130157470703),\n",
       " ('mistress', 0.5761330127716064),\n",
       " ('monarch', 0.55811607837677),\n",
       " ('auntie', 0.5540074110031128),\n",
       " ('queens', 0.5527753829956055)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7863554954528809),\n",
       " ('atrocious', 0.7167770862579346),\n",
       " ('dreadful', 0.7135769724845886),\n",
       " ('horrible', 0.7100532054901123),\n",
       " ('horrid', 0.7070688605308533),\n",
       " ('abysmal', 0.7010518312454224),\n",
       " ('horrendous', 0.6881977319717407),\n",
       " ('appalling', 0.6536094546318054),\n",
       " ('lousy', 0.6527203321456909),\n",
       " ('crappy', 0.6411494016647339)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
